{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-triumph",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "from tensorflow.python.eager import backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulated_gradients(gradients, step_gradients, batch_multiplier):\n",
    "    if gradients is None:\n",
    "        gradients = step_gradients\n",
    "    else:\n",
    "        for i, g in enumerate(step_gradients):\n",
    "            gradients[i] += g\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indie-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_batchsize(model, batch_size, batch_multiplier, num_epochs):\n",
    "    \n",
    "    # Generate training data set\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((trainX, trainY))\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    \n",
    "    # Initilization\n",
    "    opt = SGD(lr=MIN_LR, momentum=0.9) # Instantiate a loss function.\n",
    "    loss_fn = tf.keras.losses.CategoricalCrossentropy() # Instantiate a loss function.\n",
    "    \n",
    "    # Initialize Counters and Loggers\n",
    "    train_counter = 0\n",
    "    loss_history_epochs = []\n",
    "    acc_history_epochs = []\n",
    "    val_loss_history_epochs = []\n",
    "    val_acc_history_epochs = []\n",
    "    \n",
    "    # Train on specified device\n",
    "    with tf.device('/GPU:0'):\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "\n",
    "            ## Progress Bar\n",
    "            print(\"\\nepoch {}/{}\".format(epoch+1,num_epochs))\n",
    "            pb_i = Progbar(num_training_samples // batch_size + 1, verbose=1, interval=0.08)\n",
    "            ##\n",
    "\n",
    "            # Initialization\n",
    "            model.reset_metrics() # Reset metrics at the beginning of each epoch\n",
    "            acc_gradients = None\n",
    "            batch_counter = 0\n",
    "            train_logs = None\n",
    "\n",
    "            # Start the current epoch\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "                # Open a GradientTape to record the operations run\n",
    "                # during the forward pass, which enables auto-differentiation.\n",
    "                with backprop.GradientTape()  as tape:\n",
    "\n",
    "                    # Run the forward pass of the layer.\n",
    "                    # The operations that the layer applies\n",
    "                    # to its inputs are going to be recorded\n",
    "                    # on the GradientTape.\n",
    "                    y_pred = model(x_batch_train, training=True) \n",
    "\n",
    "                    # Compute the loss value for this minibatch.\n",
    "                    loss_value = loss_fn(y_batch_train, y_pred) / batch_multiplier\n",
    "\n",
    "                # Use the gradient tape to automatically retrieve\n",
    "                # the gradients of the trainable variables with respect to the loss.\n",
    "                step_gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "                # accumulate gradients\n",
    "                acc_gradients = accumulated_gradients(acc_gradients, \n",
    "                                                      step_gradients, \n",
    "                                                      batch_multiplier)\n",
    "                # Update\n",
    "                if (batch_counter == 0):        # batch_multiplier of gradients accumulated      \n",
    "                    opt.apply_gradients(zip(acc_gradients, model.trainable_variables))\n",
    "                    acc_gradients = None\n",
    "                    batch_counter = batch_multiplier\n",
    "                \n",
    "\n",
    "                # update metrics\n",
    "                model.compiled_metrics.update_state(y_batch_train, y_pred)\n",
    "                train_logs = {m.name : float(m.result()) for m in model.metrics}\n",
    "                values= [('loss', loss_value*batch_multiplier), ('acc', train_logs['accuracy'])]\n",
    "                pb_i.add(1, values=values)\n",
    "\n",
    "                # Log step\n",
    "                train_counter += 1  \n",
    "                batch_counter -= 1\n",
    "\n",
    "            ## Log result of current epoch\n",
    "            # Calculate average training loss in this epoch\n",
    "            loss_history_epochs.append(float(loss_value) * batch_multiplier)\n",
    "            acc_history_epochs.append(train_logs['accuracy'])\n",
    "\n",
    "            # Validation                \n",
    "            val_logs = model.evaluate(testX, testY, \n",
    "                                      batch_size=32, \n",
    "                                      steps=10, \n",
    "                                      return_dict=True, \n",
    "                                      verbose=0)\n",
    "            val_logs = {'val_' + name: val for name, val in val_logs.items()}\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            # Log validation results\n",
    "            val_loss_history_epochs.append(val_logs['val_loss'])\n",
    "            val_acc_history_epochs.append(val_logs['val_accuracy'])\n",
    "            \n",
    "            print(val_logs)\n",
    "            \n",
    "    return {'train': [loss_history_epochs, acc_history_epochs], \n",
    "            'validation': [val_loss_history_epochs, val_acc_history_epochs]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(lr=MIN_LR):\n",
    "    \n",
    "    # to complete\n",
    "    model = MiniGoogLeNet.build(...)\n",
    "    model.compile(...)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-electron",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = [[64, 1], \n",
    "           [128 , 1],\n",
    "           [256, 1], \n",
    "           [512, 1],\n",
    "           [1024, 1],\n",
    "           [2048, 1],\n",
    "           [2048, 2],\n",
    "           [2048, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "for [batch_size, batch_multiplier] in batches:\n",
    "    \n",
    "    # Train Model\n",
    "    model = create_model(...)\n",
    "    history = train_with_batchsize(model, batch_size=batch_size, batch_multiplier=batch_multiplier, num_epochs=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
